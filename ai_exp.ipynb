{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOb2Kwo9QWYF5ciKtAYNBd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digitaldaimyo/FastSAM/blob/main/ai_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write and Zip"
      ],
      "metadata": {
        "id": "1rJDL3Fp0Owm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('ai_exp', exist_ok=True)"
      ],
      "metadata": {
        "id": "wf7yOs3r2T7L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-688tim_qI3W",
        "outputId": "591d35a7-40a1-45bb-d275-57a55e40865f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ai_exp/tap.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile ai_exp/tap.py\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Set, Any, Optional, Literal, Iterable, Union, Mapping\n",
        "import contextvars\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Global tap stack shared across all Tap.Modules (thread/Task-local via contextvars)\n",
        "_TAP_STACK: contextvars.ContextVar[List[\"Tap._State\"]] = contextvars.ContextVar(\"_TAP_STACK\", default=[])\n",
        "\n",
        "\n",
        "class Tap:\n",
        "    \"\"\"\n",
        "    Unified forward-only tap system for model introspection.\n",
        "\n",
        "    Key ideas:\n",
        "      - Cheap capture by default (device='keep'): `tap()` only detaches.\n",
        "      - Explicit materialization via CaptureHandle.snapshot('cpu'|'clone'|'keep'|'meta').\n",
        "      - Global, contextvars-backed tap stack: no submodule rebinding; nesting composes.\n",
        "    \"\"\"\n",
        "\n",
        "    @dataclass\n",
        "    class _State:\n",
        "        active: bool = False\n",
        "        data: Dict[str, List[Any]] = field(default_factory=dict)\n",
        "        allowed_keys: Optional[Set[str]] = None\n",
        "        # What Module.tap does at write-time\n",
        "        capture_device: Literal[\"keep\", \"cpu\", \"clone\", \"meta\"] = \"keep\"\n",
        "\n",
        "    class Context:\n",
        "        class _CaptureHandle(Mapping):\n",
        "            def __init__(self, data_ref: Dict[str, List[Any]]):\n",
        "                self._data = data_ref\n",
        "\n",
        "            def __getitem__(self, k): return self._data[k]\n",
        "            def __iter__(self): return iter(self._data)\n",
        "            def __len__(self): return len(self._data)\n",
        "\n",
        "            def snapshot(self, tensor: Literal[\"keep\",\"clone\",\"cpu\",\"meta\"] = \"keep\") -> Dict[str, List[Any]]:\n",
        "                \"\"\"\n",
        "                Take a deep copy for safe, post-context use.\n",
        "                - 'keep': leave tensors as-is (already detached by tap()).\n",
        "                - 'clone': clone tensors on current device.\n",
        "                - 'cpu':   clone tensors to CPU.\n",
        "                - 'meta':  replace tensors with {shape,dtype,device} dicts.\n",
        "                \"\"\"\n",
        "                def tf(t: torch.Tensor):\n",
        "                    if tensor == \"keep\":\n",
        "                        return t\n",
        "                    if tensor == \"clone\":\n",
        "                        return t.clone()\n",
        "                    if tensor == \"cpu\":\n",
        "                        return t.detach().cpu().clone()\n",
        "                    if tensor == \"meta\":\n",
        "                        return {\"shape\": tuple(t.shape), \"dtype\": str(t.dtype), \"device\": str(t.device)}\n",
        "                    raise ValueError(\"tensor must be one of {'keep','clone','cpu','meta'}\")\n",
        "\n",
        "                def walk(o):\n",
        "                    if isinstance(o, torch.Tensor): return tf(o)\n",
        "                    if isinstance(o, dict):  return {k: walk(v) for k, v in o.items()}\n",
        "                    if isinstance(o, list):  return [walk(v) for v in o]\n",
        "                    if isinstance(o, tuple): return tuple(walk(v) for v in o)\n",
        "                    if isinstance(o, set):   return {walk(v) for v in o}\n",
        "                    try: return deepcopy(o)\n",
        "                    except Exception: return o\n",
        "\n",
        "                return walk(self._data)\n",
        "\n",
        "            __call__ = snapshot  # small convenience alias\n",
        "            @property\n",
        "            def live(self): return self._data\n",
        "\n",
        "            def getone(self, key: str, idx: int = -1):\n",
        "                lst = self._data.get(key, [])\n",
        "                return lst[idx] if lst else None\n",
        "\n",
        "        def __init__(\n",
        "            self,\n",
        "            keys: Union[str, Iterable[str], None] = None,\n",
        "            device: Literal['keep','cpu','clone','meta'] = 'keep'\n",
        "        ):\n",
        "            self.keys = set(keys) if keys is not None and not isinstance(keys, str) else {keys} if isinstance(keys, str) else None\n",
        "            self.device = device\n",
        "            self.state = Tap._State(active=True, allowed_keys=self.keys, capture_device=self.device)\n",
        "            self._token = None\n",
        "\n",
        "        def __enter__(self) -> 'Tap.Context._CaptureHandle':\n",
        "            stack = _TAP_STACK.get()\n",
        "            new_stack = stack + [self.state]\n",
        "            self._token = _TAP_STACK.set(new_stack)\n",
        "            # Fresh buffer each entry\n",
        "            self.state.data.clear()\n",
        "            return Tap.Context._CaptureHandle(self.state.data)\n",
        "\n",
        "        def __exit__(self, exc_type, exc_value, traceback):\n",
        "            # Pop this state from the stack\n",
        "            stack = _TAP_STACK.get()\n",
        "            if stack and stack[-1] is self.state:\n",
        "                _TAP_STACK.set(stack[:-1])\n",
        "            else:\n",
        "                # Defensive: remove if mis-nested\n",
        "                _TAP_STACK.set([s for s in stack if s is not self.state])\n",
        "            # Clean up\n",
        "            self.state.active = False\n",
        "            self.state.allowed_keys = None\n",
        "            self.state.capture_device = 'keep'\n",
        "            self.state.data.clear()\n",
        "\n",
        "    @staticmethod\n",
        "    def tap(*keys: str):\n",
        "        if not keys:\n",
        "            raise ValueError(\"Tap.tap requires at least one key\")\n",
        "        def decorator(fn):\n",
        "            fn._tap_keys = list(keys)\n",
        "            return fn\n",
        "        return decorator\n",
        "\n",
        "    class _KeyRecorder(dict):\n",
        "        \"\"\"Records keys written via __setitem__ without storing values.\"\"\"\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self._seen = set()\n",
        "        def __setitem__(self, k, v):\n",
        "            self._seen.add(k)\n",
        "        def keys(self):\n",
        "            return list(self._seen)\n",
        "\n",
        "    class Module(nn.Module):\n",
        "        def tap(self, key: str, value: Any):\n",
        "            stack = _TAP_STACK.get()\n",
        "            if not stack:\n",
        "                return\n",
        "            st: Tap._State = stack[-1]\n",
        "            if not st.active or (st.allowed_keys is not None and key not in st.allowed_keys):\n",
        "                return\n",
        "            if key not in st.data:\n",
        "                st.data[key] = []\n",
        "            # Tensor handling per capture policy\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                value = value.detach()\n",
        "                if st.capture_device == 'cpu':\n",
        "                    value = value.to('cpu', copy=True)\n",
        "                elif st.capture_device == 'clone':\n",
        "                    value = value.clone()\n",
        "                elif st.capture_device == 'meta':\n",
        "                    value = {\"shape\": tuple(value.shape), \"dtype\": str(value.dtype), \"device\": str(value.device)}\n",
        "                # 'keep' => already detached; store reference\n",
        "            st.data[key].append(value)\n",
        "\n",
        "        def capture(\n",
        "            self,\n",
        "            keys: Union[str, Iterable[str], None] = None,\n",
        "            device: Literal['keep','cpu','clone','meta'] = 'keep'\n",
        "        ) -> 'Tap.Context':\n",
        "            # Context is global; not bound to this module specifically.\n",
        "            return Tap.Context(keys=keys, device=device)\n",
        "\n",
        "        def list_taps(self, *args, **kwargs) -> List[str]:\n",
        "            \"\"\"\n",
        "            Run a forward pass with a key recorder to discover emitted tap keys.\n",
        "            Accepts arbitrary forward(*args, **kwargs).\n",
        "            \"\"\"\n",
        "            # Push a temporary state using a KeyRecorder\n",
        "            state = Tap._State(active=True, allowed_keys=None, capture_device='keep')\n",
        "            recorder = Tap._KeyRecorder()\n",
        "            state.data = recorder  # type: ignore[assignment]\n",
        "            stack = _TAP_STACK.get()\n",
        "            _TAP_STACK.set(stack + [state])\n",
        "            try:\n",
        "                self(*args, **kwargs)\n",
        "                return list(recorder.keys())\n",
        "            finally:\n",
        "                # Pop the temporary state\n",
        "                cur = _TAP_STACK.get()\n",
        "                if cur and cur[-1] is state:\n",
        "                    _TAP_STACK.set(cur[:-1])\n",
        "                else:\n",
        "                    _TAP_STACK.set([s for s in cur if s is not state])\n",
        "\n",
        "        @classmethod\n",
        "        def available_taps(cls) -> List[str]:\n",
        "            seen, taps = set(), []\n",
        "            for base in reversed(cls.__mro__):\n",
        "                if not issubclass(base, Tap.Module): continue\n",
        "                for obj in base.__dict__.values():\n",
        "                    if callable(obj) and hasattr(obj, \"_tap_keys\"):\n",
        "                        for key in obj._tap_keys:  # type: ignore[attr-defined]\n",
        "                            if key not in seen:\n",
        "                                seen.add(key); taps.append(key)\n",
        "            return taps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ai_exp/utils.py\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib, torch\n",
        "\n",
        "from typing import Dict, Any\n",
        "\n",
        "def stable_hash(obj: Dict[str, Any]) -> str:\n",
        "    canon = json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))\n",
        "    return hashlib.sha1(canon.encode()).hexdigest()[:10]\n",
        "\n",
        "def save_ckpt_atomic(path: str, payload: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Durable atomic write:\n",
        "      - write to tmp\n",
        "      - fsync file\n",
        "      - os.replace\n",
        "      - fsync directory\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    with open(tmp, \"wb\") as f:\n",
        "        torch.save(payload, f)\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n",
        "    os.replace(tmp, path)\n",
        "    dir_fd = os.open(os.path.dirname(path), os.O_DIRECTORY)\n",
        "    try:\n",
        "        os.fsync(dir_fd)\n",
        "    finally:\n",
        "        os.close(dir_fd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Z1l6C4qJi8",
        "outputId": "72622fe3-a07e-4733-8353-a83ee07352c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ai_exp/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile ai_exp/policy.py\n",
        "from __future__ import annotations\n",
        "from typing import Callable, List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "import traceback as _tb\n",
        "\n",
        "@dataclass\n",
        "class Context:\n",
        "    epoch: int\n",
        "    step: int\n",
        "    model: Any\n",
        "    device: Any\n",
        "    cfg: Any\n",
        "    train_loader: Any\n",
        "    eval_loader: Any\n",
        "    run: Any\n",
        "    logger: Any\n",
        "\n",
        "class Policy:\n",
        "    _REGISTRY: Dict[str, Callable] = {}\n",
        "\n",
        "    @classmethod\n",
        "    def register(cls, name: str):\n",
        "        def wrap(fn: Callable):\n",
        "            if name in cls._REGISTRY:\n",
        "                raise ValueError(f\"Collector '{name}' already registered.\")\n",
        "            cls._REGISTRY[name] = fn\n",
        "            fn._collector_name = name  # type: ignore[attr-defined]\n",
        "            return fn\n",
        "        return wrap\n",
        "\n",
        "    @classmethod\n",
        "    def get(cls, name: str) -> Callable:\n",
        "        if name not in cls._REGISTRY:\n",
        "            raise KeyError(f\"Collector '{name}' not found.\")\n",
        "        return cls._REGISTRY[name]\n",
        "\n",
        "    @classmethod\n",
        "    def list_collectors(cls) -> List[str]:\n",
        "        return sorted(cls._REGISTRY.keys())\n",
        "\n",
        "    def __init__(self, name: str = \"default\"):\n",
        "        self.name = name\n",
        "        self._config: Dict[str, Any] = {\"every_epoch\": [], \"every_k_epochs\": []}\n",
        "\n",
        "    def _normalize_every_epoch(self, items: List[Any]) -> List[Dict[str, Any]]:\n",
        "        norm = []\n",
        "        for it in items:\n",
        "            if isinstance(it, str):\n",
        "                self.get(it)\n",
        "                norm.append({\"name\": it, \"params\": {}})\n",
        "            elif isinstance(it, dict) and \"name\" in it:\n",
        "                self.get(it[\"name\"])\n",
        "                norm.append({\"name\": str(it[\"name\"]), \"params\": dict(it.get(\"params\", {}))})\n",
        "            else:\n",
        "                raise ValueError(\"Invalid every_epoch spec\")\n",
        "        return norm\n",
        "\n",
        "    def _normalize_every_k(self, items: List[Any]) -> List[Dict[str, Any]]:\n",
        "        norm = []\n",
        "        for it in items:\n",
        "            if not (isinstance(it, dict) and \"name\" in it and \"k\" in it):\n",
        "                raise ValueError(\"every_k_epochs must have {'name','k',...}\")\n",
        "            self.get(it[\"name\"])\n",
        "            norm.append({\"name\": str(it[\"name\"]), \"k\": int(it[\"k\"]), \"params\": dict(it.get(\"params\", {}))})\n",
        "        return norm\n",
        "\n",
        "    def configure(self, *, every_epoch=None, every_k_epochs=None):\n",
        "        if every_epoch is not None:\n",
        "            self._config[\"every_epoch\"] = self._normalize_every_epoch(every_epoch)\n",
        "        if every_k_epochs is not None:\n",
        "            self._config[\"every_k_epochs\"] = self._normalize_every_k(every_k_epochs)\n",
        "        return self  # chaining\n",
        "\n",
        "    def to_manifest(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"name\": self.name,\n",
        "            \"every_epoch\": [{\"name\": e[\"name\"], \"params\": e.get(\"params\", {})} for e in self._config[\"every_epoch\"]],\n",
        "            \"every_k_epochs\": [{\"name\": e[\"name\"], \"k\": e[\"k\"], \"params\": e.get(\"params\", {})} for e in self._config[\"every_k_epochs\"]],\n",
        "            \"available_collectors\": self.list_collectors(),\n",
        "        }\n",
        "\n",
        "    def _safe_call(self, ctx: Context, name: str, **params):\n",
        "        try:\n",
        "            fn = self.get(name)\n",
        "            fn(ctx, **params)\n",
        "        except Exception as e:\n",
        "            ctx.logger.log_event(\n",
        "                \"collector_error\",\n",
        "                epoch=ctx.epoch, name=name,\n",
        "                msg=str(e),\n",
        "                tb=\"\".join(_tb.format_exc(limit=5))\n",
        "            )\n",
        "\n",
        "    def run_collectors(self, ctx: Context, edge_epoch_tick: bool = True):\n",
        "        for spec in self._config[\"every_epoch\"]:\n",
        "            self._safe_call(ctx, spec[\"name\"], **spec.get(\"params\", {}))\n",
        "\n",
        "        for spec in self._config[\"every_k_epochs\"]:\n",
        "            if ctx.epoch % spec[\"k\"] == 0:\n",
        "                self._safe_call(ctx, spec[\"name\"], **spec.get(\"params\", {}))\n",
        "\n",
        "        if edge_epoch_tick and ctx.epoch + 1 == ctx.cfg.train.max_epochs:\n",
        "            ctx.logger.log_event(\"run_complete\", epoch=ctx.epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO58tde3qJt8",
        "outputId": "6eaad140-99bb-46ba-ef84-f8fe086299a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ai_exp/policy.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile ai_exp/logger.py\n",
        "from __future__ import annotations\n",
        "import os, json, time, numpy as np\n",
        "import torch\n",
        "\n",
        "class RunLogger:\n",
        "    def __init__(self, run):\n",
        "        self.run_dir = run.run_dir\n",
        "        self.art_dir = os.path.join(run.run_dir, \"artifacts\")\n",
        "        self.log_dir = os.path.join(run.run_dir, \"logs\")\n",
        "        os.makedirs(self.art_dir, exist_ok=True)\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "\n",
        "        self._epoch_f = open(os.path.join(self.log_dir, \"epoch.jsonl\"), \"a\", buffering=1)\n",
        "        self._events_f = open(os.path.join(self.log_dir, \"events.jsonl\"), \"a\", buffering=1)\n",
        "        self._lock_path = os.path.join(run.run_dir, \"_LOCK\")\n",
        "        # Write PID to help detect stale locks\n",
        "        with open(self._lock_path, \"w\") as f:\n",
        "            try:\n",
        "                f.write(str(os.getpid()))\n",
        "            except Exception:\n",
        "                f.write(\"unknown\")\n",
        "\n",
        "    def _print(self, msg: str):\n",
        "        try:\n",
        "            print(msg)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def log_epoch(self, to_console: bool = False, **kv):\n",
        "        rec = {\"t\": time.time(), **kv}\n",
        "        self._epoch_f.write(json.dumps(rec) + \"\\n\")\n",
        "        if to_console:\n",
        "            ep = rec.get(\"epoch\")\n",
        "            if isinstance(ep, int):\n",
        "                msg = f\"[epoch {ep:03d}]\"\n",
        "            else:\n",
        "                msg = \"[epoch ?]\"\n",
        "            lr = rec.get(\"lr\")\n",
        "            tl = rec.get(\"train_loss\")\n",
        "            l1 = rec.get(\"eval_l1\")\n",
        "            if isinstance(lr, (int, float)): msg += f\" lr={lr:.2e}\"\n",
        "            if isinstance(tl, (int, float)): msg += f\" train={tl:.4f}\"\n",
        "            if isinstance(l1, (int, float)): msg += f\" evalL1={l1:.4f}\"\n",
        "            self._print(msg)\n",
        "\n",
        "    def log_event(self, kind: str, to_console: bool = False, **kv):\n",
        "        rec = {\"t\": time.time(), \"kind\": kind, **kv}\n",
        "        self._events_f.write(json.dumps(rec) + \"\\n\")\n",
        "        if to_console:\n",
        "            parts = [f\"{k}={v}\" for k, v in sorted(kv.items())]\n",
        "            self._print(f\"[event:{kind}] {' '.join(parts)}\")\n",
        "\n",
        "    def save_npz(self, name: str, to_console: bool = False, **arrays):\n",
        "        path = os.path.join(self.art_dir, name)\n",
        "        np.savez_compressed(path, **{\n",
        "            k: (v.detach().cpu().numpy() if torch.is_tensor(v) else v)\n",
        "            for k, v in arrays.items()\n",
        "        })\n",
        "        if to_console:\n",
        "            self._print(f\"[artifact] saved {name}\")\n",
        "        return path\n",
        "\n",
        "    def close(self):\n",
        "        try:\n",
        "            self._epoch_f.close()\n",
        "            self._events_f.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            os.remove(self._lock_path)\n",
        "        except Exception:\n",
        "            pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyO-mHShqJ2_",
        "outputId": "44e1ff9e-87d4-4294-d87a-b986aba23cd8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ai_exp/logger.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile ai_exp/run.py\n",
        "from __future__ import annotations\n",
        "import os, time, json, torch, random, numpy as np\n",
        "from typing import Optional\n",
        "from dataclasses import asdict\n",
        "from .utils import stable_hash, save_ckpt_atomic\n",
        "from .logger import RunLogger\n",
        "from .policy import Policy, Context\n",
        "\n",
        "class Run:\n",
        "    def __init__(self, cfg, tag: Optional[str] = None, policy: Optional[Policy] = None):\n",
        "        self.cfg = cfg\n",
        "        self.tag = (tag or \"\").strip()\n",
        "        self.policy = policy\n",
        "        self.logger: Optional[RunLogger] = None\n",
        "        self.started = False\n",
        "        self.device: Optional[torch.device] = None\n",
        "        self.dtype: Optional[torch.dtype] = None\n",
        "        self.timestamp: Optional[str] = None\n",
        "        self.requested_device: Optional[str] = None  # preserve original intent\n",
        "\n",
        "    def _resolve_device(self) -> torch.device:\n",
        "        req = getattr(self.cfg.train, \"device\", \"auto\")\n",
        "        self.requested_device = req\n",
        "        if req == \"auto\":\n",
        "            dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            resolved = dev\n",
        "        else:\n",
        "            resolved = req\n",
        "        # record the resolved choice but keep the requested for manifest\n",
        "        self.cfg.train.device = resolved\n",
        "        return torch.device(resolved)\n",
        "\n",
        "    def _resolve_dtype(self) -> torch.dtype:\n",
        "        p = self.cfg.train.precision.lower()\n",
        "        if p == \"fp32\": return torch.float32\n",
        "        if p == \"fp16\": return torch.float16\n",
        "        if p == \"bf16\": return torch.bfloat16\n",
        "        raise ValueError(f\"Unknown precision: {self.cfg.train.precision}\")\n",
        "\n",
        "    def _seed_everything(self):\n",
        "        sd = self.cfg.train.seed\n",
        "        random.seed(sd); np.random.seed(sd); torch.manual_seed(sd)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(sd)\n",
        "        # Determinism knobs\n",
        "        torch.use_deterministic_algorithms(self.cfg.run.deterministic)\n",
        "        # Optional: TF32 off for stricter determinism\n",
        "        try:\n",
        "            torch.backends.cuda.matmul.allow_tf32 = False\n",
        "            torch.backends.cudnn.allow_tf32 = False\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Optional CUDA workspace for full determinism (PyTorch will warn if needed)\n",
        "        os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
        "\n",
        "    def _is_writable(self, path: str) -> bool:\n",
        "        try:\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "            p = os.path.join(path, \".touch\")\n",
        "            with open(p, \"w\") as f: f.write(\"ok\")\n",
        "            os.remove(p)\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _resolve_paths(self):\n",
        "        root = self.cfg.run.log_root if self._is_writable(self.cfg.run.log_root) else self.cfg.run.local_fallback\n",
        "        self.timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        if not hasattr(self.cfg, \"exp_hash\") or not self.cfg.exp_hash:\n",
        "            self.cfg.finalize()\n",
        "        tag = self.tag or \"default\"\n",
        "        rd = os.path.join(root, tag, self.cfg.exp_hash[:8], self.timestamp)\n",
        "        for sub in (\"\", \"artifacts\", \"logs\", \"checkpoints\"):\n",
        "            os.makedirs(os.path.join(rd, sub), exist_ok=True)\n",
        "        self.cfg.run.run_dir = rd\n",
        "        return rd\n",
        "\n",
        "    def _write_manifest(self):\n",
        "        cuda_info = {}\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                cuda_info = {\n",
        "                    \"device_name\": torch.cuda.get_device_name(0),\n",
        "                    \"cc\": \".\".join(map(str, torch.cuda.get_device_capability(0))),\n",
        "                    \"cuda\": torch.version.cuda,\n",
        "                }\n",
        "            except Exception:\n",
        "                cuda_info = {}\n",
        "        man = {\n",
        "            \"schema\": 1,\n",
        "            \"created_at\": time.time(),\n",
        "            \"timestamp\": self.timestamp,\n",
        "            \"tag\": self.tag,\n",
        "            \"exp_hash\": self.cfg.exp_hash,\n",
        "            \"cfg\": asdict(self.cfg),\n",
        "            \"policy\": (self.policy.to_manifest() if self.policy else None),\n",
        "            \"paths\": {\n",
        "                \"run_dir\": self.cfg.run.run_dir,\n",
        "                \"artifacts\": os.path.join(self.cfg.run.run_dir, \"artifacts\"),\n",
        "                \"logs\": os.path.join(self.cfg.run.run_dir, \"logs\"),\n",
        "                \"checkpoints\": os.path.join(self.cfg.run.run_dir, \"checkpoints\"),\n",
        "            },\n",
        "            \"env\": {\"torch\": torch.__version__, **cuda_info},\n",
        "            \"requested_device\": self.requested_device,\n",
        "            \"resolved_device\": self.cfg.train.device,\n",
        "        }\n",
        "        with open(os.path.join(self.cfg.run.run_dir, \"manifest.json\"), \"w\") as f:\n",
        "            json.dump(man, f, indent=2, default=str)\n",
        "\n",
        "    def _print_header(self):\n",
        "        header = [\n",
        "            f\"Run started\",\n",
        "            f\" tag: {self.tag}\",\n",
        "            f\" device: {self.device.type}\",\n",
        "            f\" precision: {str(self.dtype).split('.')[-1]}\",\n",
        "            f\" seed: {self.cfg.train.seed}\",\n",
        "            f\" exp_hash: {self.cfg.exp_hash[:8]}\",\n",
        "            f\" timestamp: {self.timestamp}\",\n",
        "            f\" run_dir: {self.cfg.run.run_dir}\",\n",
        "            f\" checkpoints: {os.path.join(self.cfg.run.run_dir, 'checkpoints')}\",\n",
        "            f\" artifacts: {os.path.join(self.cfg.run.run_dir, 'artifacts')}\",\n",
        "        ]\n",
        "        print(\"\\n\".join(header))\n",
        "\n",
        "    def start(self):\n",
        "        assert not self.started, \"Run already started\"\n",
        "        self.cfg.finalize()\n",
        "        self.device = self._resolve_device()\n",
        "        self.dtype = self._resolve_dtype()\n",
        "        self._seed_everything()\n",
        "        self._resolve_paths()\n",
        "        self._write_manifest()\n",
        "        self.logger = RunLogger(self)\n",
        "        self._print_header()\n",
        "        self.started = True\n",
        "        return self.device, self.logger\n",
        "\n",
        "    def finish(self):\n",
        "        if self.logger: self.logger.close()\n",
        "        self.started = False\n",
        "\n",
        "    def apply_policy(self, ctx: Context, edge_epoch_tick: bool = True):\n",
        "        if self.policy is not None:\n",
        "            self.policy.run_collectors(ctx, edge_epoch_tick)\n",
        "\n",
        "    @property\n",
        "    def run_dir(self) -> str:\n",
        "        assert self.cfg.run.run_dir\n",
        "        return self.cfg.run.run_dir\n",
        "\n",
        "    @property\n",
        "    def checkpoints_dir(self) -> str:\n",
        "        return os.path.join(self.run_dir, \"checkpoints\")\n",
        "\n",
        "    @property\n",
        "    def artifacts_dir(self) -> str:\n",
        "        return os.path.join(self.run_dir, \"artifacts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wMzxoS1qJ_Y",
        "outputId": "93088bef-f507-41ab-ab42-4b4d8dd82ccb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ai_exp/run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile ai_exp/collectors.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import io\n",
        "from typing import Dict, Any, Iterable, Optional, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from .policy import Policy, Context\n",
        "from .tap import Tap\n",
        "\n",
        "\n",
        "def _flatten_named_params(model: torch.nn.Module, names: Optional[Iterable[str]] = None):\n",
        "    \"\"\"\n",
        "    Yields (name, param) pairs. If `names` is provided, filters to those names (exact match or suffix match).\n",
        "    \"\"\"\n",
        "    wanted = set(names) if names is not None else None\n",
        "    for n, p in model.named_parameters():\n",
        "        if wanted is None:\n",
        "            yield n, p\n",
        "        else:\n",
        "            if n in wanted or any(n.endswith(\"/\" + w) or n.endswith(\".\" + w) for w in wanted):\n",
        "                yield n, p\n",
        "\n",
        "\n",
        "def _safe_tensor_to_np(t: torch.Tensor):\n",
        "    try:\n",
        "        return t.detach().cpu().numpy()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _summarize_model_text(model: torch.nn.Module) -> str:\n",
        "    # Param counts\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    non_trainable = total - trainable\n",
        "\n",
        "    # Small one-line-per-submodule tree\n",
        "    lines = [f\"Total params: {total:,} (trainable={trainable:,}, frozen={non_trainable:,})\"]\n",
        "    for name, module in model.named_modules():\n",
        "        if name == \"\":\n",
        "            continue\n",
        "        params = sum(p.numel() for p in module.parameters(recurse=False))\n",
        "        lines.append(f\"{name}: {module.__class__.__name__} (params={params:,})\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "\n",
        "\n",
        "def _write_text_artifact(ctx: Context, name: str, text: str) -> str:\n",
        "    path = os.path.join(ctx.run.artifacts_dir, name)\n",
        "    _ensure_dir(path)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    ctx.logger.log_event(\"artifact_text\", name=name)\n",
        "    return path\n",
        "\n",
        "\n",
        "def _stack_or_meta(seq):\n",
        "    \"\"\"\n",
        "    Try to stack a list of tensors. If incompatible, return a meta dict.\n",
        "    \"\"\"\n",
        "    if not seq:\n",
        "        return None\n",
        "    if all(isinstance(x, torch.Tensor) for x in seq):\n",
        "        shapes = [tuple(x.shape) for x in seq]\n",
        "        dtypes = [str(x.dtype) for x in seq]\n",
        "        if len(set(shapes)) == 1 and len(set(dtypes)) == 1:\n",
        "            try:\n",
        "                return torch.stack(seq, dim=0)\n",
        "            except Exception:\n",
        "                pass\n",
        "        # fallthrough: meta\n",
        "        return {\n",
        "            \"shapes\": shapes,\n",
        "            \"dtypes\": dtypes,\n",
        "            \"count\": len(seq),\n",
        "        }\n",
        "    # Mixed content: return counts/meta\n",
        "    return {\"types\": [type(x).__name__ for x in seq], \"count\": len(seq)}\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Collectors\n",
        "# ----------------------\n",
        "\n",
        "@Policy.register(\"grad_norms\")\n",
        "def collect_grad_norms(ctx: Context, group_by: str = \"global\"):\n",
        "    \"\"\"\n",
        "    Saves gradient L2 norms.\n",
        "      group_by: \"global\" (single number) or \"per_param\"\n",
        "    \"\"\"\n",
        "    norms = []\n",
        "    per_param = {}\n",
        "    for n, p in ctx.model.named_parameters():\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        g = p.grad.detach()\n",
        "        val = g.norm(2)\n",
        "        norms.append(val)\n",
        "        if group_by == \"per_param\":\n",
        "            per_param[n] = val\n",
        "\n",
        "    if norms:\n",
        "        gsum = torch.stack(norms).norm(2)\n",
        "        arrs: Dict[str, Any] = {\"grad_norm_global\": gsum}\n",
        "        if group_by == \"per_param\":\n",
        "            # Convert to a structured array (names truncated for npz keys)\n",
        "            for k, v in per_param.items():\n",
        "                arrs[f\"grad_norm__{k}\"] = v\n",
        "        ctx.logger.save_npz(f\"grad_norms_epoch{ctx.epoch:04d}.npz\", **arrs)\n",
        "        ctx.logger.log_event(\"grad_norms\", epoch=ctx.epoch, global_l2=float(gsum.detach().cpu()))\n",
        "    else:\n",
        "        ctx.logger.log_event(\"grad_norms\", epoch=ctx.epoch, note=\"no_grads\")\n",
        "\n",
        "\n",
        "@Policy.register(\"param_hists\")\n",
        "def collect_param_histograms(ctx: Context, names: Optional[Iterable[str]] = None, bins: int = 64, max_params: int = 256):\n",
        "    \"\"\"\n",
        "    Saves compact histograms for parameters. If `names` is None, samples up to `max_params` parameters.\n",
        "    \"\"\"\n",
        "    arrs = {}\n",
        "    count = 0\n",
        "    for n, p in _flatten_named_params(ctx.model, names):\n",
        "        if p.numel() == 0:\n",
        "            continue\n",
        "        # Sample to a budget to avoid giant artifacts\n",
        "        if names is None and count >= max_params:\n",
        "            break\n",
        "        x = _safe_tensor_to_np(p)\n",
        "        if x is None:\n",
        "            continue\n",
        "        # Flatten and histogram\n",
        "        x = x.ravel()\n",
        "        try:\n",
        "            h, edges = np.histogram(x, bins=bins)\n",
        "            arrs[f\"{n}__hist\"] = h\n",
        "            arrs[f\"{n}__edges\"] = edges\n",
        "            count += 1\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if arrs:\n",
        "        ctx.logger.save_npz(f\"param_hists_epoch{ctx.epoch:04d}.npz\", **arrs)\n",
        "        ctx.logger.log_event(\"param_hists\", epoch=ctx.epoch, params=count)\n",
        "    else:\n",
        "        ctx.logger.log_event(\"param_hists\", epoch=ctx.epoch, note=\"no_params_or_failed\")\n",
        "\n",
        "\n",
        "@Policy.register(\"gpu_mem\")\n",
        "def collect_gpu_memory(ctx: Context):\n",
        "    \"\"\"\n",
        "    Logs CUDA memory (in MB) if available.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            alloc = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "            reserv = torch.cuda.memory_reserved() / (1024 ** 2)\n",
        "            ctx.logger.log_event(\"gpu_mem\", epoch=ctx.epoch, allocated_mb=round(alloc, 2), reserved_mb=round(reserv, 2))\n",
        "        except Exception as e:\n",
        "            ctx.logger.log_event(\"gpu_mem_error\", epoch=ctx.epoch, msg=str(e))\n",
        "    else:\n",
        "        ctx.logger.log_event(\"gpu_mem\", epoch=ctx.epoch, note=\"cpu_only\")\n",
        "\n",
        "\n",
        "@Policy.register(\"model_summary\")\n",
        "def collect_model_summary(ctx: Context, filename: str = \"model_summary.txt\"):\n",
        "    \"\"\"\n",
        "    Writes a simple text summary + param counts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = _summarize_model_text(ctx.model)\n",
        "        _write_text_artifact(ctx, filename, text)\n",
        "        ctx.logger.log_event(\"model_summary\", epoch=ctx.epoch, file=filename)\n",
        "    except Exception as e:\n",
        "        ctx.logger.log_event(\"model_summary_error\", epoch=ctx.epoch, msg=str(e))\n",
        "\n",
        "\n",
        "@Policy.register(\"eval_sample\")\n",
        "def collect_eval_sample(ctx: Context, batches: int = 1, filename: str = \"eval_sample_epoch{epoch:04d}.npz\"):\n",
        "    \"\"\"\n",
        "    Runs a few eval batches and saves outputs (and targets if present) to NPZ.\n",
        "    Tries to be generic: supports dataloaders yielding:\n",
        "      - (inputs, targets, *rest), or\n",
        "      - dict with 'inputs'/'targets' keys, or\n",
        "      - just inputs.\n",
        "    \"\"\"\n",
        "    if ctx.eval_loader is None:\n",
        "        ctx.logger.log_event(\"eval_sample\", epoch=ctx.epoch, note=\"no_eval_loader\")\n",
        "        return\n",
        "\n",
        "    model = ctx.model\n",
        "    model_device = next((p.device for p in model.parameters() if p is not None), ctx.device)\n",
        "    saved = False\n",
        "    arrays: Dict[str, Any] = {}\n",
        "    k = 0\n",
        "\n",
        "    def _split_batch(b) -> Tuple[torch.Tensor, Optional[Any]]:\n",
        "        if isinstance(b, dict):\n",
        "            x = b.get(\"inputs\", b.get(\"x\", b.get(\"data\")))\n",
        "            y = b.get(\"targets\", b.get(\"y\", None))\n",
        "            return x, y\n",
        "        if isinstance(b, (tuple, list)) and len(b) >= 1:\n",
        "            x = b[0]\n",
        "            y = b[1] if len(b) >= 2 else None\n",
        "            return x, y\n",
        "        return b, None\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for b in ctx.eval_loader:\n",
        "            x, y = _split_batch(b)\n",
        "            try:\n",
        "                if isinstance(x, torch.Tensor):\n",
        "                    x = x.to(model_device, non_blocking=True)\n",
        "                out = model(x)\n",
        "                # Save a small slice to keep artifact sizes tame\n",
        "                if isinstance(out, torch.Tensor):\n",
        "                    arrays[f\"out_{k}\"] = out.detach().cpu()\n",
        "                elif isinstance(out, (list, tuple)) and out and isinstance(out[0], torch.Tensor):\n",
        "                    arrays[f\"out_{k}\"] = out[0].detach().cpu()\n",
        "                if isinstance(x, torch.Tensor):\n",
        "                    arrays[f\"in_{k}\"] = x.detach().cpu()\n",
        "                if isinstance(y, torch.Tensor):\n",
        "                    arrays[f\"tgt_{k}\"] = y.detach().cpu()\n",
        "                saved = True\n",
        "            except Exception as e:\n",
        "                ctx.logger.log_event(\"eval_sample_error\", epoch=ctx.epoch, msg=str(e))\n",
        "            k += 1\n",
        "            if k >= batches:\n",
        "                break\n",
        "\n",
        "    if saved:\n",
        "        fname = filename.format(epoch=ctx.epoch)\n",
        "        ctx.logger.save_npz(fname, **arrays)\n",
        "        ctx.logger.log_event(\"eval_sample\", epoch=ctx.epoch, batches=k, file=fname)\n",
        "    else:\n",
        "        ctx.logger.log_event(\"eval_sample\", epoch=ctx.epoch, note=\"nothing_saved\")\n",
        "\n",
        "\n",
        "@Policy.register(\"tap_eval\")\n",
        "def collect_tap_eval(ctx: Context, keys: Optional[Iterable[str]] = None, filename: str = \"tap_eval_epoch{epoch:04d}.npz\", device: str = \"keep\"):\n",
        "    \"\"\"\n",
        "    If the model inherits from Tap.Module, run one eval batch under a capture and persist tap values.\n",
        "    - Tries to stack per-key lists of tensors; if not possible, saves meta.\n",
        "    - `device`: 'keep'|'cpu'|'clone'|'meta' for capture-time write behavior.\n",
        "    \"\"\"\n",
        "    if not isinstance(ctx.model, Tap.Module):\n",
        "        ctx.logger.log_event(\"tap_eval\", epoch=ctx.epoch, note=\"model_not_TapModule\")\n",
        "        return\n",
        "    if ctx.eval_loader is None:\n",
        "        ctx.logger.log_event(\"tap_eval\", epoch=ctx.epoch, note=\"no_eval_loader\")\n",
        "        return\n",
        "\n",
        "    model: Tap.Module = ctx.model\n",
        "    model_device = next((p.device for p in model.parameters() if p is not None), ctx.device)\n",
        "\n",
        "    # Get one batch\n",
        "    try:\n",
        "        batch = next(iter(ctx.eval_loader))\n",
        "    except Exception as e:\n",
        "        ctx.logger.log_event(\"tap_eval_error\", epoch=ctx.epoch, msg=f\"loader_iter: {e}\")\n",
        "        return\n",
        "\n",
        "    def _get_inputs(b):\n",
        "        if isinstance(b, dict):\n",
        "            return b.get(\"inputs\", b.get(\"x\", b.get(\"data\", b)))\n",
        "        if isinstance(b, (tuple, list)) and len(b) >= 1:\n",
        "            return b[0]\n",
        "        return b\n",
        "\n",
        "    x = _get_inputs(batch)\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x = x.to(model_device, non_blocking=True)\n",
        "\n",
        "    arrays: Dict[str, Any] = {}\n",
        "    meta: Dict[str, Any] = {}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        with model.capture(keys=keys, device=device) as cap:\n",
        "            try:\n",
        "                _ = model(x)\n",
        "            except Exception as e:\n",
        "                ctx.logger.log_event(\"tap_eval_error\", epoch=ctx.epoch, msg=f\"forward: {e}\")\n",
        "                return\n",
        "            snap = cap.snapshot(\"cpu\")  # materialize to CPU for saving\n",
        "\n",
        "    # Try to convert per-key lists into stackable arrays\n",
        "    for k, seq in snap.items():\n",
        "        st = _stack_or_meta(seq)\n",
        "        if st is None:\n",
        "            continue\n",
        "        if isinstance(st, torch.Tensor):\n",
        "            arrays[f\"{k}\"] = st  # tensor will be moved to np by save_npz\n",
        "        else:\n",
        "            meta[k] = st\n",
        "\n",
        "    fname = filename.format(epoch=ctx.epoch)\n",
        "    if arrays:\n",
        "        ctx.logger.save_npz(fname, **arrays)\n",
        "    # Save meta (if any) as a sidecar JSON\n",
        "    if meta:\n",
        "        sidecar = fname.replace(\".npz\", \".meta.json\")\n",
        "        path = os.path.join(ctx.run.artifacts_dir, sidecar)\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        import json\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, indent=2)\n",
        "    ctx.logger.log_event(\"tap_eval\", epoch=ctx.epoch, keys=list(snap.keys()), file=fname, meta=(\"yes\" if meta else \"no\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHbg06SAqKIH",
        "outputId": "fd16b02a-75df-4b26-825a-6adc30591030"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ai_exp/collectors.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac8b699d",
        "outputId": "15c4d062-c489-49e8-afb7-b44e94815ecf"
      },
      "source": [
        "import os\n",
        "\n",
        "init_file = 'ai_exp/__init__.py'\n",
        "with open(init_file, 'w') as f:\n",
        "    pass\n",
        "print(f\"Created {init_file}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ai_exp/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8cdc497",
        "outputId": "ba113f91-5c39-42d8-e53c-4488b2e56adc"
      },
      "source": [
        "\n",
        "package_name = 'ai_exp'\n",
        "requirements = ['numpy', 'torch']\n",
        "requirements_file_path = os.path.join(package_name, \"requirements.txt\")\n",
        "\n",
        "import os\n",
        "\n",
        "init_file = requirements_file_path\n",
        "with open(requirements_file_path, 'w') as f:\n",
        "    for req in requirements:\n",
        "        f.write(f\"{req}\\n\")\n",
        "\n",
        "print(f\"Created {requirements_file_path}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ai_exp/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "461a33b5",
        "outputId": "6d8c2da8-e712-4081-f3df-c22a8996fcfe"
      },
      "source": [
        "import shutil\n",
        "# Zip the 'ai_exp' directory\n",
        "shutil.make_archive(package_name, 'zip', root_dir='.', base_dir=package_name)\n",
        "print(f\"Created {package_name}.zip\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ai_exp.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract"
      ],
      "metadata": {
        "id": "dM_B5LKR0LJj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdcbed1e",
        "outputId": "f81108cb-8b2d-46f6-ccf1-b69a280e6fb9"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = 'ai_exp.zip' # Replace with the path to your zip file\n",
        "# Extract to a subdirectory named 'ai_exp_extracted' inside the current directory\n",
        "extract_dir = 'ai_exp_extracted/ai_exp_files' # Modify this to your desired extraction path\n",
        "\n",
        "if os.path.exists(zip_file_path):\n",
        "    # Create the extraction directory if it doesn't exist\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all contents to the specified directory\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"Extracted {zip_file_path} to {extract_dir}\")\n",
        "else:\n",
        "    print(f\"Error: {zip_file_path} not found.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted ai_exp.zip to ai_exp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "iKzkCRSu0V37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- cell 1: setup & config ---\n",
        "from __future__ import annotations\n",
        "import os, json, time\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# your framework (ensure xai_exp/ is importable)\n",
        "from ai_exp.tap import Tap\n",
        "from ai_exp.policy import Policy, Context\n",
        "from ai_exp.collectors import *  # registers collectors\n",
        "from ai_exp.run import Run\n",
        "from ai_exp.utils import stable_hash, save_ckpt_atomic\n",
        "\n",
        "@dataclass\n",
        "class TrainCfg:\n",
        "    device: str = \"auto\"            # \"auto\" | \"cpu\" | \"cuda\"\n",
        "    precision: str = \"fp32\"         # \"fp32\" | \"fp16\" | \"bf16\"\n",
        "    seed: int = 1337\n",
        "    batch_size: int = 256\n",
        "    max_epochs: int = 5\n",
        "    lr: float = 0.2\n",
        "    momentum: float = 0.9\n",
        "    weight_decay: float = 5e-4\n",
        "    amp: bool = True                # use autocast if fp16/bf16 on cuda\n",
        "\n",
        "@dataclass\n",
        "class RunCfg:\n",
        "    log_root: str = \"/content/xai_runs\"\n",
        "    local_fallback: str = \"/content/xai_runs\"\n",
        "    deterministic: bool = True\n",
        "    run_dir: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class DataCfg:\n",
        "    num_workers: int = 2\n",
        "\n",
        "@dataclass\n",
        "class Cfg:\n",
        "    train: TrainCfg\n",
        "    run: RunCfg\n",
        "    data: DataCfg\n",
        "    exp_hash: str = \"\"\n",
        "\n",
        "    def finalize(self):\n",
        "        canon = {\n",
        "            \"train\": asdict(self.train),\n",
        "            \"run\": {k:v for k,v in asdict(self.run).items() if k != \"run_dir\"},\n",
        "            \"data\": asdict(self.data),\n",
        "        }\n",
        "        self.exp_hash = stable_hash(canon)\n",
        "\n",
        "cfg = Cfg(train=TrainCfg(), run=RunCfg(), data=DataCfg())"
      ],
      "metadata": {
        "id": "udahSYxWuOMI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- cell 2: dataset & loaders ---\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "def make_loaders(cfg: Cfg):\n",
        "    # Standard CIFAR-10 stats\n",
        "    MEAN = (0.4914, 0.4822, 0.4465)\n",
        "    STD  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    train_tf = T.Compose([\n",
        "        T.RandomCrop(32, padding=4),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(MEAN, STD),\n",
        "    ])\n",
        "    eval_tf = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(MEAN, STD),\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"/content/data\", train=True, download=True, transform=train_tf)\n",
        "    eval_set  = torchvision.datasets.CIFAR10(root=\"/content/data\", train=False, download=True, transform=eval_tf)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=cfg.train.batch_size, shuffle=True,\n",
        "                              num_workers=cfg.data.num_workers, pin_memory=True)\n",
        "    eval_loader  = DataLoader(eval_set, batch_size=512, shuffle=False,\n",
        "                              num_workers=cfg.data.num_workers, pin_memory=True)\n",
        "    return train_loader, eval_loader\n",
        "\n",
        "train_loader, eval_loader = make_loaders(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ZTJMi_uOWd",
        "outputId": "f6548e72-b4c2-4b49-93dd-bf79a32531c1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 170M/170M [00:12<00:00, 14.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- cell 3: model with taps ---\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
        "        self.down  = None\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_ch)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        idt = x\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        if self.down is not None:\n",
        "            idt = self.down(idt)\n",
        "        x = F.relu(x + idt)\n",
        "        return x\n",
        "\n",
        "class CIFARNet(Tap.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer1 = BasicBlock(32, 64, stride=2)\n",
        "        self.layer2 = BasicBlock(64, 128, stride=2)\n",
        "        self.layer3 = BasicBlock(128, 128, stride=1)\n",
        "        self.pool   = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc     = nn.Linear(128, num_classes)\n",
        "\n",
        "        # declare tap keys for discoverability\n",
        "        @Tap.tap(\"stem_in\",\"stem_out\",\"l1_out\",\"l2_out\",\"l3_out\",\"head_in\",\"logits\")\n",
        "        def _taps(): pass\n",
        "        self._taps = _taps\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.tap(\"stem_in\", x)\n",
        "        x = self.stem(x)\n",
        "        self.tap(\"stem_out\", x)\n",
        "        x = self.layer1(x)\n",
        "        self.tap(\"l1_out\", x)\n",
        "        x = self.layer2(x)\n",
        "        self.tap(\"l2_out\", x)\n",
        "        x = self.layer3(x)\n",
        "        self.tap(\"l3_out\", x)\n",
        "        h = self.pool(x).flatten(1)\n",
        "        self.tap(\"head_in\", h)\n",
        "        logits = self.fc(h)\n",
        "        self.tap(\"logits\", logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "l637olP_ubYJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- cell 4: policy, run, training/eval ---\n",
        "# Collectors policy exercising all mechanisms\n",
        "policy = Policy(\"cifar10_demo\").configure(\n",
        "    every_epoch=[\n",
        "        \"gpu_mem\",\n",
        "        {\"name\":\"grad_norms\",\"params\":{\"group_by\":\"global\"}},\n",
        "        {\"name\":\"param_hists\",\"params\":{\"bins\":64,\"max_params\":64}},\n",
        "        {\"name\":\"eval_sample\",\"params\":{\"batches\":1}},\n",
        "        \"model_summary\",\n",
        "        {\"name\":\"tap_eval\",\"params\":{\"device\":\"keep\"}},  # taps via cheap capture\n",
        "    ],\n",
        "    every_k_epochs=[\n",
        "        {\"name\":\"param_hists\",\"k\":3,\"params\":{\"bins\":128,\"max_params\":128}},\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = CIFARNet(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "run = Run(cfg=cfg, tag=\"cifar10_tap_demo\", policy=policy)\n",
        "device, logger = run.start()\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer & simple cosine schedule\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=cfg.train.lr,\n",
        "                            momentum=cfg.train.momentum, weight_decay=cfg.train.weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.train.max_epochs)\n",
        "\n",
        "# AMP setup (optional)\n",
        "use_amp = (cfg.train.amp and device.type == \"cuda\" and cfg.train.precision.lower() in {\"fp16\",\"bf16\"})\n",
        "autocast_dtype = torch.float16 if cfg.train.precision.lower()==\"fp16\" else torch.bfloat16\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp and autocast_dtype==torch.float16)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    l1 = 0.0\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=use_amp, dtype=autocast_dtype):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.numel()\n",
        "            l1 += logits.abs().mean().item()\n",
        "    return correct / max(1,total), l1 / max(1,len(loader))\n",
        "\n",
        "global_step = 0\n",
        "for epoch in range(cfg.train.max_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp, dtype=autocast_dtype):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        running_loss += float(loss.item())\n",
        "        global_step += 1\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_loss = running_loss / max(1, len(train_loader))\n",
        "    acc, l1 = evaluate(model, eval_loader, device)\n",
        "\n",
        "    logger.log_epoch(to_console=True,\n",
        "                     epoch=epoch,\n",
        "                     lr=scheduler.get_last_lr()[0],\n",
        "                     train_loss=epoch_loss,\n",
        "                     eval_acc=acc,\n",
        "                     eval_l1=l1)\n",
        "\n",
        "    # Run collectors (drops artifacts incl. tap snapshots)\n",
        "    ctx = Context(epoch=epoch, step=global_step, model=model, device=device,\n",
        "                  cfg=cfg, train_loader=train_loader, eval_loader=eval_loader,\n",
        "                  run=run, logger=logger)\n",
        "    run.apply_policy(ctx, edge_epoch_tick=True)\n",
        "\n",
        "    # Save checkpoint atomically\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optim\": optimizer.state_dict(),\n",
        "        \"sched\": scheduler.state_dict(),\n",
        "        \"cfg\": asdict(cfg),\n",
        "    }\n",
        "    save_ckpt_atomic(os.path.join(run.checkpoints_dir, f\"epoch_{epoch:04d}.pt\"), ckpt)\n",
        "\n",
        "run.finish()\n",
        "print(\"Run dir:\", cfg.run.run_dir)\n",
        "print(\"Artifacts:\", os.listdir(os.path.join(cfg.run.run_dir, \"artifacts\")))\n",
        "print(\"Logs:\", os.listdir(os.path.join(cfg.run.run_dir, \"logs\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV-NpFZQuozH",
        "outputId": "973f3ebb-d37d-4686-d252-29f760131cf4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run started\n",
            " tag: cifar10_tap_demo\n",
            " device: cuda\n",
            " precision: float32\n",
            " seed: 1337\n",
            " exp_hash: fc1c5f3c\n",
            " timestamp: 20251031-200645\n",
            " run_dir: /content/xai_runs/cifar10_tap_demo/fc1c5f3c/20251031-200645\n",
            " checkpoints: /content/xai_runs/cifar10_tap_demo/fc1c5f3c/20251031-200645/checkpoints\n",
            " artifacts: /content/xai_runs/cifar10_tap_demo/fc1c5f3c/20251031-200645/artifacts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4032867052.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and autocast_dtype==torch.float16)\n",
            "/tmp/ipython-input-4032867052.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp, dtype=autocast_dtype):\n",
            "/tmp/ipython-input-4032867052.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=use_amp, dtype=autocast_dtype):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 000] lr=1.81e-01 train=1.6608 evalL1=1.7620\n",
            "[epoch 001] lr=1.31e-01 train=1.2030 evalL1=2.3557\n",
            "[epoch 002] lr=6.91e-02 train=0.9396 evalL1=2.3013\n",
            "[epoch 003] lr=1.91e-02 train=0.7401 evalL1=2.8518\n",
            "[epoch 004] lr=0.00e+00 train=0.5997 evalL1=2.9267\n",
            "Run dir: /content/xai_runs/cifar10_tap_demo/fc1c5f3c/20251031-200645\n",
            "Artifacts: ['param_hists_epoch0002.npz', 'eval_sample_epoch0003.npz', 'tap_eval_epoch0001.npz', 'eval_sample_epoch0000.npz', 'model_summary.txt', 'param_hists_epoch0003.npz', 'tap_eval_epoch0000.npz', 'grad_norms_epoch0003.npz', 'param_hists_epoch0001.npz', 'eval_sample_epoch0001.npz', 'param_hists_epoch0000.npz', 'eval_sample_epoch0004.npz', 'grad_norms_epoch0004.npz', 'grad_norms_epoch0001.npz', 'tap_eval_epoch0003.npz', 'grad_norms_epoch0002.npz', 'grad_norms_epoch0000.npz', 'param_hists_epoch0004.npz', 'eval_sample_epoch0002.npz', 'tap_eval_epoch0004.npz', 'tap_eval_epoch0002.npz']\n",
            "Logs: ['events.jsonl', 'epoch.jsonl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- cell 5: analysis utilities ---\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "\n",
        "print(\"Available taps (class):\", CIFARNet.available_taps())\n",
        "\n",
        "# list_taps via a dry forward\n",
        "dummy = torch.randn(2,3,32,32).to(device)\n",
        "print(\"list_taps(*):\", model.list_taps(dummy))\n",
        "\n",
        "# Cheap capture during eval, then explicit CPU snapshot\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    xb, yb = next(iter(eval_loader))\n",
        "    xb = xb.to(device)\n",
        "    with model.capture(device='keep') as cap:\n",
        "        _ = model(xb)\n",
        "    snap = cap.snapshot('cpu')\n",
        "\n",
        "print(\"Captured keys:\", list(snap.keys()))\n",
        "for k, seq in list(snap.items())[:4]:\n",
        "    if isinstance(seq, list) and seq and torch.is_tensor(seq[0]):\n",
        "        print(f\"  {k:10s}: {len(seq)} tensors; first shape={tuple(seq[0].shape)}\")\n",
        "\n",
        "# Inspect tap_eval artifact\n",
        "tap_npzs = sorted(glob(os.path.join(cfg.run.run_dir, \"artifacts\", \"tap_eval_epoch*.npz\")))\n",
        "print(\"tap_eval artifacts:\", [os.path.basename(p) for p in tap_npzs][-3:])\n",
        "if tap_npzs:\n",
        "    npz = np.load(tap_npzs[-1], allow_pickle=True)\n",
        "    print(\"npz keys:\", list(npz.keys())[:8])\n",
        "    k0 = list(npz.keys())[0]\n",
        "    print(k0, \"->\", npz[k0].shape, npz[k0].dtype)\n",
        "\n",
        "# Look at a few events\n",
        "with open(os.path.join(cfg.run.run_dir, \"logs\", \"events.jsonl\"), \"r\") as f:\n",
        "    lines = f.readlines()[-10:]\n",
        "print(\"\\nLast 10 events:\")\n",
        "for ln in lines:\n",
        "    print(ln.strip())\n",
        "\n",
        "# Show a couple histogram keys\n",
        "hist_npzs = sorted(glob(os.path.join(cfg.run.run_dir, \"artifacts\", \"param_hists_epoch*.npz\")))\n",
        "if hist_npzs:\n",
        "    npz = np.load(hist_npzs[-1], allow_pickle=True)\n",
        "    some = [k for k in npz.files if k.endswith(\"__hist\")][:5]\n",
        "    print(\"\\nHistogram keys:\", some)"
      ],
      "metadata": {
        "id": "8_v09HwBucbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yWdy1BmuOd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gr874MP5t2kI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}